{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3422c1",
   "metadata": {},
   "source": [
    "# Project - Supervised Classifcation - Loan Modelling\n",
    "\n",
    "\n",
    "## Background and Context\n",
    "\n",
    "AllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\n",
    "\n",
    "A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\n",
    "\n",
    "We need to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.\n",
    "\n",
    "## Objective\n",
    "\n",
    "To predict whether a liability customer will buy a personal loan or not.\n",
    "Which variables are most significant.\n",
    "Which segment of customers should be targeted more.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. What are the key factors influencing whether a liability customer will buy a personal loan or not?\n",
    "2. Is there a good predictive model so that we can increase the conversion rate of the campaign? \n",
    "3. What does the performance assessment look like for such a model?\n",
    "\n",
    "\n",
    "## Data Dictionary\n",
    "* ID: Customer ID\n",
    "* Age: Customer’s age in completed years\n",
    "* Experience: #years of professional experience\n",
    "* Income: Annual income of the customer (in thousand dollars)\n",
    "* ZIP Code: Home Address ZIP code.\n",
    "* Family: the Family size of the customer\n",
    "* CCAvg: Average spending on credit cards per month (in thousand dollars)\n",
    "* Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional\n",
    "* Mortgage: Value of house mortgage if any. (in thousand dollars)\n",
    "* Personal_Loan: Did this customer accept the personal loan offered in the last campaign?\n",
    "* Securities_Account: Does the customer have securities account with the bank?\n",
    "* CD_Account: Does the customer have a certificate of deposit (CD) account with the bank?\n",
    "* Online: Do customers use internet banking facilities?\n",
    "* CreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad17944",
   "metadata": {},
   "source": [
    "## Import necessary libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to help with reading and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Removes the limit from the number of displayed columns and rows.\n",
    "# This is so that we can see the entire dataframe when we print it\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# We are setting the random seed via np.random.seed so that\n",
    "# we get the same random results every time\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loan modelling CSV file\n",
    "data = pd.read_csv('Loan_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {data.shape[0]} and number of columns: {data.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the last 5 rows of data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking column names, datatypes and number of non-null values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba490ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b176aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bfa79",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "* ID column can be dropped as it is a serial number and does not contain any useful information\n",
    "* There are no null or missing values in the data. \n",
    "* There are no duplicate records.\n",
    "* All column types are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not interested in the ID column so we are going to drop the ID column\n",
    "data.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Summary of the data\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5587d",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* *Age*: Varies from 23 to 67 years with mean and median around 45 years. This data range looks good.\n",
    "* *Experience*: Ranges from -3 to 43 years. Negative value for professional experience does not make sense. It could be data input errors. We shall inspect the values and take corrective steps.\n",
    "* *Income*: Minimum is 8K and max is 224K. Mean is around 73K and median 64K. This data range looks good and no correction is required.\n",
    "* *ZIPCode*: All zip codes are from 90005 to 96651. Hence data appears to be from state of California. We need to figure out a strategy to convert this data to a set of categorical values. \n",
    "* *Family*: Customer family size with min 1 and maximum 4 and mean/median around 2. This data range looks good.\n",
    "* *CCAvg*: Average spending on credit cards per month varies from 0 to 10K with mean of 2k and median of 1.5K. This data range also looks good. \n",
    "* *Education*: Education column is a category column values are 1(Undergraduate), 2(Graduate) or 3(Advanced/Professional).\n",
    "* *Mortgage*: Value of house mortgage also seems normal and varies from 0 to 635K. Median value of morgage is 0 since most of the customers will not have mortgage.\n",
    "* *Personal_Loan*: This is our target variable and is of type binary 0 or 1(accepted the personal loan offer). \n",
    "* *Securities_Account*, *CD_Account*, *Online*, *CreditCard*: All these columns are again binary type categorical variables with 0 and 1 values. Data summary shows no abnormality in the data values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f70c8",
   "metadata": {},
   "source": [
    "Other than Age, Experience, Income, CCAvg and Mortgage all other features of categorial type based on the data definitions. Lets check the range of values for these categorical variables to make sure they are categorical and the size of the unique values in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ZIPCode', 'Family', 'Education', 'Personal_Loan', 'Securities_Account', 'CD_Account', 'Online', 'CreditCard']\n",
    "for col in cat_cols:\n",
    "    print('>> Domain of: ', col)\n",
    "    cat = data[col].value_counts(dropna=False).sort_values(ascending=False)\n",
    "    print(cat)\n",
    "    print('-------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf9e9e",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "* ZIP codes have 467 unique values. We are going to convert ZIP codes to a more manageable categorical type.\n",
    "* Other categorical values are as described in the data definition hence we are good with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e572b",
   "metadata": {},
   "source": [
    "### Fixing negative Experience Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794417cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the rows where we have negative Professional Experience \n",
    "len(data[data.Experience < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcfad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check all the rows, since there are 52\n",
    "data[data.Experience < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2450c",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* It appears that where Experience is negative, Age of the customers are within 23 to 29 years, which means customers are younger and have less professional experience. So we can safely conclude that negative sign is just an input error and in order to fix this, we can just convert the values to positive and no other transformation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver the negative values to positive with a lambda function\n",
    "data['Experience'] = data['Experience'].apply(lambda x: x if x > 0 else -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure there are no negative values\n",
    "len(data[data.Experience < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of rows having Experience > 40 also looks good as Age of these customers are over 60.\n",
    "data[data.Experience > 40].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eeb889",
   "metadata": {},
   "source": [
    "### Treatment of ZIPCode column\n",
    "\n",
    "We will use [usezipcode](https://pypi.org/project/uszipcode/) to convert zip code to major city, county and check the range of values in each category. For this purpose we will write a function to get Major City or County from a zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install uszipcode\n",
    "from uszipcode import SearchEngine\n",
    "zip_search = SearchEngine(simple_zipcode=True)\n",
    "def zip_to_name(code, name):\n",
    "    \"\"\"\n",
    "    Convert zip code to another attribute, such as City or County\n",
    "    code: ZIP code, name: major_city, county or state\n",
    "    Returns name of the attribute. If major_city then it will return the Major City of the given  zip code.\n",
    "    \"\"\"\n",
    "    zipcode = zip_search.by_zipcode(code)\n",
    "    return zipcode.to_dict()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function with sample ZIP code\n",
    "print('Major city for zip: 22102: ', zip_to_name(22102, 'major_city'))\n",
    "print('County for zip: 22102: ', zip_to_name(22102, 'county'))\n",
    "print('State for zip: 22102: ', zip_to_name(22102, 'state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add three columns Major_City, County and State from Zip code.\n",
    "data['Major_City'] = data['ZIPCode'].map(lambda zcode: zip_to_name(zcode, 'major_city'))\n",
    "data['County'] = data['ZIPCode'].map(lambda zcode: zip_to_name(zcode, 'county'))\n",
    "data['State'] = data['ZIPCode'].map(lambda zcode: zip_to_name(zcode, 'state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look of random sample of 10 rows\n",
    "data[['ZIPCode', 'Major_City', 'County', 'State']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fc89d",
   "metadata": {},
   "source": [
    "Check the value ranges of the newly added columns we do not want to drop NA values in case there are invalid zip codes for which there will be no city, county or state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value ranges of the newly added columns, \n",
    "for col in ['Major_City', 'County', 'State']:\n",
    "    print('>> Domain of: ', col)\n",
    "    cat = data[col].value_counts(dropna=False).sort_values(ascending=False)\n",
    "    print(cat)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c0fd0",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "* There are invalid ZIP codes it seems in 34 rows, as 34 rows have NaN in State and County columns. \n",
    "* Major_City: If we convert zip to Major_City then it will still be 245 unique values which are very high. It will make the model very complex.\n",
    "* County: Unique number of all counties are also over 30. But we can use this column and group it by median income of its population to segregate customers into smaller groups which will also be a good fit for our machine learning models.\n",
    "* State: If we convert zip to state then it will be only one or two values. We may loose significance of the ZIP column altogether.\n",
    "\n",
    "Hence our decision would be to go with the County column and merge them to a smaller number of groups based on median income of the couties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c032c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at sample of 10 rows where where county is missing.\n",
    "data[data['County'].isnull()].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83964476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check invalid zipcodes based on the County = None\n",
    "data[data['County'].isnull()]['ZIPCode'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183f54b",
   "metadata": {},
   "source": [
    "Checking with [USPS](https://tools.usps.com/zip-code-lookup.htm?citybyzipcode) it seems the above zip codes are invalid and do not exist. We will user \"Other\" in place of invalid Counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tools.usps.com/zip-code-lookup.htm?citybyzipcode\n",
    "data['County'].fillna('Other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a799e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column that will have median income group by County\n",
    "data['Income_Median'] = data.groupby('County')['Income'].transform(lambda x: x.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c91fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sample of the data\n",
    "data[['County', 'Income', 'Income_Median']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and print the unique median income values, so that we can use pd.cut() to bin the median incomes \n",
    "# to managable ranges\n",
    "print(sorted(data['Income_Median'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median income grouped by County varies from 25K to 86K, but there are no values between 25K to 44K, \n",
    "# hence we plan to bin the median income besed on below ranges.\n",
    "bins = [20, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# Labels of the bins are one less than the bins size, our bins would be like income less than 40K (MI_<40), \n",
    "# income between 40K to 50K, will be denoted by MI_<50, income between 50K to 60K, will be denoted by MI_<60 etc.\n",
    "labels = ['MI_<'+ str(i) for i in bins[1:]]\n",
    "print(bins)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde05762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'County_MI' county median income to collect the binned data using pd.cut()\n",
    "data['County_MI'] = pd.cut(data['Income_Median'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check random sample of the data\n",
    "data[['ZIPCode', 'County', 'Income_Median', 'County_MI']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see all counties are categorized into six median income groups, we are going to use this new column \n",
    "# in place of ZIPCode\n",
    "data['County_MI'].value_counts(ascending=False, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have added few columns, now lets check all the columns and we are going to drop the columns that are not necessary.\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep a back up of ZIPCode','Major_City', 'County', 'State', 'Income_Median' data\n",
    "zip_county_mi = data[['ZIPCode','County','Income_Median', 'County_MI']].copy()\n",
    "\n",
    "# Drop 'ZIPCode','Major_City', 'County', 'State', 'Income_Median' and only keep County_MI column\n",
    "data.drop(['ZIPCode','Major_City', 'County', 'State', 'Income_Median'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949babf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 rows of the modified data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafa105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f9801",
   "metadata": {},
   "source": [
    "Data preparation and feature engineering is complete, at the end we have 13 columns. ZIPCode has been replaced with County_MI. During the univariate analysis if we find more issues then we should revisit and treat the feature accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37110327",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd1670",
   "metadata": {},
   "source": [
    "For making the univariate analysis easier, we define below functions that will help us to plot both non-categorical columns such as Age, Income etc. as box and hist plots and categorical columns such as Family, Education, Online etc. as count plot with percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41024217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "def box_hist_plot(data, feature, figsize=(12, 7), bins=10):\n",
    "    \"\"\"\n",
    "    This will show a box and hist plot in a column alignment, For hist plot kde is set to True\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    \"\"\"\n",
    "    # creating the 2 subplots\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  \n",
    "    # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.boxplot(data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\")  \n",
    "    # For hist plot\n",
    "    sns.histplot(data=data, x=feature, kde=True, bins=bins, ax=ax_hist2) \n",
    "    # Add mean to the histogram\n",
    "    ax_hist2.axvline(data[feature].mean(), color=\"green\", linestyle=\"--\") \n",
    "    # Add median to the histogram\n",
    "    ax_hist2.axvline(data[feature].median(), color=\"black\", linestyle=\"-\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create bar plot with percent labels\n",
    "def bar_perc_plot(data, feature):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate the fig width dynamically\n",
    "    total = len(data[feature]) \n",
    "    count = data[feature].nunique()\n",
    "    plt.figure(figsize=(count + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        # Sort the bars from high to low\n",
    "        order=data[feature].value_counts().sort_values(ascending=False).index,\n",
    "    )\n",
    "\n",
    "    for bar in ax.patches:\n",
    "        # percentage of each class of the category\n",
    "        label = \"{:.2f}%\".format(100 * bar.get_height() / total)  \n",
    "        x = bar.get_x() + bar.get_width() / 2  # width of the plot\n",
    "        y = bar.get_height()  # height of the plot\n",
    "\n",
    "        # annotate the percentage\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        ) \n",
    "\n",
    "    plt.show();  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91ed7c",
   "metadata": {},
   "source": [
    "### Observations on Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8769d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age\n",
    "box_hist_plot(data, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222b4a8",
   "metadata": {},
   "source": [
    "> Age is normally distributed and no outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41da06",
   "metadata": {},
   "source": [
    "### Observations on Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience\n",
    "box_hist_plot(data, 'Experience')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77077183",
   "metadata": {},
   "source": [
    "> Professional experience of customers are normally distributed and there are no outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf738a",
   "metadata": {},
   "source": [
    "### Observations on Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2703065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income\n",
    "box_hist_plot(data, 'Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed40b2",
   "metadata": {},
   "source": [
    "> Income is right skewed with mean and median are separated by around 10K. Although this is not an issue for Logistic Regression or Decision Tree models. Below text is from the Logistic regression Literature \n",
    "\n",
    "> \"Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms – particularly regarding linearity, normality, homoscedasticity, and measurement level.\"\n",
    "\n",
    "> Hence we do not need to transform this column to Log normal or other similar transformations. We would follow the same approach for other columns as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a392d5d",
   "metadata": {},
   "source": [
    "### Observations on CCAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CCAvg\n",
    "box_hist_plot(data, 'CCAvg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1e355",
   "metadata": {},
   "source": [
    "> CCAvg has a right skew, and there are many outliers. But again outliers are not a big issue for Logistic Regression or Decision Tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466b69a",
   "metadata": {},
   "source": [
    "### Observations on Mortgage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707cdad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Mortgage\n",
    "box_hist_plot(data, 'Mortgage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571cc18",
   "metadata": {},
   "source": [
    "> Mortgage is highly right skewed, and there are many outliers. This is because most of the customers do not have Morgage as suggested by the mean which is close to 0. Only few customers have very high morgage in the range of 500K to 600K but that is not abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers having higher mortgage have higher income and are older customers. This shows data is not misleading.\n",
    "data[data.Mortgage > 500].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea21fd9a",
   "metadata": {},
   "source": [
    "### Observations on Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'Family')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd9f3e",
   "metadata": {},
   "source": [
    "> Most of the families have single member follwed by 2, 4 and 3 members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f6582",
   "metadata": {},
   "source": [
    "### Observations on Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c33766",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb04c9",
   "metadata": {},
   "source": [
    "> Most of customers are Undergraduate followed by Advanced degree and Graduate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b200cbf",
   "metadata": {},
   "source": [
    "### Observations on Personal_Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdffc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'Personal_Loan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42037a5",
   "metadata": {},
   "source": [
    "> * 90.4% of customers have not responded or signed up for Personal Loan\n",
    "> * Only 9.6% of customers have responded to the Personal Loan campaign.\n",
    "> * Conversion rate of the campaign is below 10%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043ba11",
   "metadata": {},
   "source": [
    "### Observations on Securities_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'Securities_Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d734a7",
   "metadata": {},
   "source": [
    "> A little over 10% customers have Securities account with the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ecbcc",
   "metadata": {},
   "source": [
    "### Observations on CD_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d07f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'CD_Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f002a",
   "metadata": {},
   "source": [
    "> Only around 6% customers have CD account with the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ec62f",
   "metadata": {},
   "source": [
    "### Observations on Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'Online')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1105c6",
   "metadata": {},
   "source": [
    "> Around 40% customers use Online banking but majority of customers do not use Online banking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d650999",
   "metadata": {},
   "source": [
    "### Observations on CreditCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'CreditCard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f31f4",
   "metadata": {},
   "source": [
    "> Around 29% customers use Credit cards from other banks which are not issued by AllLife Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cdbd4",
   "metadata": {},
   "source": [
    "### Observations on Customer Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_perc_plot(data, 'County_MI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b46558",
   "metadata": {},
   "source": [
    "> * More than 70% customers are from Couties where median income is between 60K to 70K. \n",
    "> * Only few customers less than 1% are from very high income counties or very low income counties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d92679",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2830bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between features\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(data.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a79af3",
   "metadata": {},
   "source": [
    "> * Age and Experience are very strongly correlated. We will remove 'Experience' feature during our model building. \n",
    "> * Income and monthly credit card average are correlated but it is not very high.\n",
    "> * Also there is a little correlation between Income and Personal Loan\n",
    "> * Otherwise most of the features are not very correlated to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9578fb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw Pair plot excluding the binary columns, we need to check the binary columns separately as they don't produce good result in pairplots\n",
    "sns.pairplot(\n",
    "    data[['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage', 'Family', 'Education', 'Personal_Loan']], \n",
    "    hue='Personal_Loan'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5b564",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "> * Here again we see Age and Experience are very strongly correlated. \n",
    "> * Interesting to see that customers who have resonded to the Personal Loan campaign occupy different space in the pair plots.\n",
    "> * Domain space of Personal Loan customers are towards higher Income, CCAvg, Mortgage, Family members and Education\n",
    "> * Personal Loan customers belong to all Age and Experience in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb3626",
   "metadata": {},
   "source": [
    "### Distribution of Personal Loan with Non-Categorical features\n",
    "\n",
    "Ignoring the outliers let's check the distribution of Personal Loan takers based on the non-categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data[\n",
    "    [\n",
    "        \"Age\",\n",
    "        \"Experience\",\n",
    "        \"Income\",\n",
    "        \"CCAvg\",\n",
    "        \"Mortgage\"\n",
    "    ]\n",
    "].columns.tolist()\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, variable in enumerate(cols):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    sns.boxplot(data = data, x=\"Personal_Loan\", y=variable, showfliers=False)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae1983",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "> * Distribution of Personal Loan customers have higher lower bound and lower upper bound when its comes to Age, compared to its other counterpart.\n",
    "> * Not much difference with regards to Experience between customers with Personal Loan 1 and 0\n",
    "> * Income and CCAge charts clearly tells Personal Loan customers have higher Income and CCAvge in general.\n",
    "> * Although Personal Loan customers spreads across all Morgage ranges but higher mortgage customers are more Personal Loan takers in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e22bc",
   "metadata": {},
   "source": [
    "### Distribution of Personal Loan with Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data[\n",
    "    [\n",
    "        \"Family\",\n",
    "        \"Education\",\n",
    "        \"Securities_Account\",\n",
    "        \"CD_Account\",\n",
    "        \"Online\",\n",
    "        \"CreditCard\"\n",
    "    ]\n",
    "].columns.tolist()\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, variable in enumerate(cols):\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    sns.countplot(data = data, x='Personal_Loan', hue=variable)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b9a52",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "> * Personal Loan customers spread across all family sizes but percentage is higher with higher family sizes.\n",
    "> * Personal Loan customers spread across all Education qualifications but percentage is higher with higher educated customers.\n",
    "> * Higher percentage of Personal Loan customers have no Security or CD account in the bank.\n",
    "> * Only around 50% of Personal Loan customers have credit cards with other banks.\n",
    "> * Higher percentage of Personal Loan customers use Online or internet banking features of the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac45e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally check the Personal Loan customer distribution across counties\n",
    "sns.histplot(data = data, x='County_MI', hue='Personal_Loan', multiple='stack');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42f280",
   "metadata": {},
   "source": [
    "> * Most of the Personal Loan customers are from couties where median income is between 60K to 70K. But this may be misleading as most customers are also from these counties.\n",
    "> * There are no Personal Loan customers from very high income or very low income counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9dcb0",
   "metadata": {},
   "source": [
    "## Model Building for Cassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c91247",
   "metadata": {},
   "source": [
    "We are going to now build two types of classification models, first with Logistic Regression followed by Decision Tree to address the key questions. But in both cases model evaluation criteria remains same. At the end we'll compare the performance of both types of models and and recommendations for AllLife Bank's marketing department Personal Loan campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa765c86",
   "metadata": {},
   "source": [
    "### Model evaluation criterion:\n",
    "\n",
    "### Model can make wrong predictions as:\n",
    "1. Predicting a person who has taken a personal loan but in reality he/she did not.\n",
    "2. Predicting a person who did not take a personal loan but in reality he/she did.\n",
    "\n",
    "### Which case is more important? \n",
    "\n",
    "* Predicting a person who has taken a personal loan but in reality he/she did not then cost related to campaign on personal loan to the customer is wasted. Campaign cost per customer is generally not significant. \n",
    "\n",
    "* If we predict a person who did not take a personal loan but in reality he/she did then it will be loss of oppertunity for the company and this is precisly we want to avoid since we want to increase the customer base of personal loan.\n",
    "\n",
    "\n",
    "### How to reduce this loss i.e need to reduce False Negatives?\n",
    "*  `recall` should be maximized, the greater the `recall` higher the chances of identifying customers who would like to buy a personal loan product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84abd5e",
   "metadata": {},
   "source": [
    "## Model Building with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6960a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a backup of the prepared dataset\n",
    "data_bkp = data.copy()\n",
    "\n",
    "# Drop the Experience column as it is very strongly correlated with Age\n",
    "data.drop(['Experience'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60446a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdbc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build model for prediction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To get diferent metric scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    plot_confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648967c",
   "metadata": {},
   "source": [
    "#### Create Traing and Test sets\n",
    "\n",
    "We'll be using the same traing and test sets for both classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d730e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Personal_Loan'], axis=1)\n",
    "y = data['Personal_Loan']\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Splitting data in train and test sets, with stratify so that data is split in a stratified fashion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a18117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Training set : \", X_train.shape)\n",
    "print(\"Shape of test set : \", X_test.shape)\n",
    "print(\"Percentage of classes in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Percentage of classes in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15949f",
   "metadata": {},
   "source": [
    "In order to calculate different metrics and confusion matrix and promote code reuse we are going to write two utility functions as below.\n",
    "* The show_model_perf_with_threshold function will be used to check the model performance of models. \n",
    "* The show_confusion_matrix_with_threshold function will be used to plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bde81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "\n",
    "def show_model_perf_with_threshold(model, predictors, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics such as Accuracy, Recall, Precision and F1, based on the threshold specified\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    threshold: threshold for classifying the observation as class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred_prob = model.predict_proba(predictors)[:, 1]\n",
    "    pred_thres = pred_prob > threshold\n",
    "    pred = np.round(pred_thres)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1\": f1,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to plot the confusion_matrix of a classification model built using sklearn\n",
    "def show_confusion_matrix_with_threshold(model, predictors, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix, based on the threshold specified, with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    threshold: threshold for classifying the observation as class 1\n",
    "    \"\"\"\n",
    "    pred_prob = model.predict_proba(predictors)[:, 1]\n",
    "    pred_thres = pred_prob > threshold\n",
    "    y_pred = np.round(pred_thres)\n",
    "\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33021d8d",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a91421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different solvers available in Sklearn logistic regression\n",
    "# The newton-cg solver is faster for high-dimensional data\n",
    "\n",
    "# Since the Personal Loan values are unbalanced and only 9.6 percent are 1s \n",
    "# we are going to give more weight to value 1 as this is our target\n",
    "lg = LogisticRegression(solver=\"newton-cg\", random_state=1, class_weight={0: 0.1, 1: 0.9})\n",
    "model = lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the coefficients and intercept of the model\n",
    "coef_df = pd.DataFrame(\n",
    "    np.append(lg.coef_, lg.intercept_),\n",
    "    index=X_train.columns.tolist() + [\"Intercept\"],\n",
    "    columns=[\"Coefficients\"],\n",
    ")\n",
    "coef_df.sort_values(by='Coefficients', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8e23e",
   "metadata": {},
   "source": [
    "### Coefficient interpretations\n",
    "\n",
    "Logistic regression model equation is not a liner one, hence coefficients will not have direct liner/proportional contributions. But a positive coefficient will have some degree of positive influence and vice versa.\n",
    "\n",
    "* Coefficient of Age, Income, Family, CCAvg, Education, Mortgage, CD_Account are positive, increase in these will lead to increase in chances of a person taking Personal Loan it seems. \n",
    "* Coefficient of Securities_Account, Online and CreditCard are negative hence increase in these will lead to decrease in chances of a person taking Personal Loan.\n",
    "* People from Counties having median income between 60K to 70K and 70K to 80K have higher chances of taking Personal Loan than other couties based on the coefficient signs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c591bb",
   "metadata": {},
   "source": [
    "###  Converting coefficients to odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30691ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting coefficients to odds\n",
    "odds = np.exp(lg.coef_[0])\n",
    "\n",
    "# finding the percentage change\n",
    "perc_change_odds = (np.exp(lg.coef_[0]) - 1) * 100\n",
    "\n",
    "# adding the odds to a dataframe\n",
    "odd_df = pd.DataFrame({\"Odds\": odds, \"Change_odd%\": perc_change_odds}, index=X_train.columns)\n",
    "odd_df.sort_values(by='Change_odd%', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acddff",
   "metadata": {},
   "source": [
    "### Coefficient interpretations\n",
    "\n",
    "* `CD_Account`: Holding all other features constant a 1 unit change in CD_Account will increase the odds of a person taking Personal Loan by 12.27 times or more than 1000% increase in odds for the same.\n",
    "* `Income`: Holding all other features constant a 1 unit change in Income will increase the odds of a person taking Personal Loan by 1.057 times or a 5.67% increase in odds for the same.\n",
    "* `Family`: Holding all other features constant a 1 unit change in Family will increase the odds of a person taking Personal Loan by 1.73 times or a 73.40% increase in odds for the same.\n",
    "* `CreditCard`: Holding all other features constant a 1 unit change in the CreditCard will decrease the odds of a person taking Personal Loan by 0.44 times or a 55.55% decrease in odds for the same.\n",
    "\n",
    "`Interpretation for other attributes can be done similarly.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a352d2",
   "metadata": {},
   "source": [
    "#### Checking model performance on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad85413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "show_confusion_matrix_with_threshold(lg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf = show_model_perf_with_threshold(lg, X_train, y_train)\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd905a",
   "metadata": {},
   "source": [
    "#### Checking model performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "show_confusion_matrix_with_threshold(lg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce14eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf = show_model_perf_with_threshold(lg, X_test, y_test)\n",
    "print(\"Test set performance:\")\n",
    "log_reg_model_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d061306",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* Both Training and Test sets have very high accuracy which is natural as Target variable is very unbalanced.\n",
    "* Training set **Recall** is 0.9 and test set recall is 0.85, this shows the model is not overfitted, but there may be room for improvements.\n",
    "* Precision is about 0.5 for both sets. This should be fine as we are looking for a higher recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6f4a2",
   "metadata": {},
   "source": [
    "Let's compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "#### ROC-AUC on Traing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497002b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC-AUC on the Training Set\n",
    "logit_roc_auc_train = roc_auc_score(y_train, lg.predict_proba(X_train)[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_train, lg.predict_proba(X_train)[:, 1])\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2cef3",
   "metadata": {},
   "source": [
    "#### ROC-AUC on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3311104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC on the test set\n",
    "logit_roc_auc_test = roc_auc_score(y_test, lg.predict_proba(X_test)[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lg.predict_proba(X_test)[:, 1])\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_test)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ba426",
   "metadata": {},
   "source": [
    "### Model Performance Improvement\n",
    "\n",
    "Let's see if the recall score can be improved further, by changing the model threshold using AUC-ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd491b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal threshold as per AUC-ROC curve\n",
    "# The optimal cut off would be where tpr is high and fpr is low\n",
    "fpr, tpr, thresholds = roc_curve(y_train, lg.predict_proba(X_train)[:, 1])\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold_auc_roc = thresholds[optimal_idx]\n",
    "print(optimal_threshold_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e4bce",
   "metadata": {},
   "source": [
    "### Optimal threshold using AUC-ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f47c6c",
   "metadata": {},
   "source": [
    "#### Model Performance on Training set with Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "show_confusion_matrix_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_auc_roc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdeeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance for this model\n",
    "log_reg_model_train_perf_threshold_auc_roc = show_model_perf_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_auc_roc\n",
    ")\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_threshold_auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d12c43",
   "metadata": {},
   "source": [
    "#### Model Performance on Test set with Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "# creating confusion matrix\n",
    "show_confusion_matrix_with_threshold(lg, X_test, y_test, threshold=optimal_threshold_auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance for this model\n",
    "log_reg_model_test_perf_threshold_auc_roc = show_model_perf_with_threshold(\n",
    "    lg, X_test, y_test, threshold=optimal_threshold_auc_roc\n",
    ")\n",
    "print(\"Test set performance:\")\n",
    "log_reg_model_test_perf_threshold_auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e0163",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* Since the optimal threshold ~ 0.51 is close to default threshold 0.5 the scores did not change much.\n",
    "* Training set **Recall** is 0.9 and test set recall is around 0.85 - not much improvement is shown.\n",
    "* Precision and F1 scores are also similar with default threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5edfc0",
   "metadata": {},
   "source": [
    "### Let's use Precision-Recall curve and see if we can find a better threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550b24d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_scores = lg.predict_proba(X_train)[:, 1]\n",
    "prec, rec, tre = precision_recall_curve(y_train, y_scores,)\n",
    "\n",
    "\n",
    "def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_prec_recall_vs_tresh(prec, rec, tre)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1893280",
   "metadata": {},
   "source": [
    "> At threshold around 0.81 we will get equal precision and recall but taking a step back and selecting value around 0.40 will provide a higher recall and a acceptable precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the threshold\n",
    "optimal_threshold_curve = 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3283594",
   "metadata": {},
   "source": [
    "#### Model Performance on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# creating confusion matrix\n",
    "show_confusion_matrix_with_threshold(lg, X_train, y_train, threshold=optimal_threshold_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf_threshold_curve = show_model_perf_with_threshold(\n",
    "    lg, X_train, y_train, threshold=optimal_threshold_curve\n",
    ")\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_threshold_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06547af7",
   "metadata": {},
   "source": [
    "#### Model Performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921efbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "show_confusion_matrix_with_threshold(lg, X_test, y_test, threshold=optimal_threshold_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf_threshold_curve = show_model_perf_with_threshold(\n",
    "    lg, X_test, y_test, threshold=optimal_threshold_curve\n",
    ")\n",
    "print(\"Test set performance:\")\n",
    "log_reg_model_test_perf_threshold_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021839f",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* With threshold 0.40 we are getting a little higher score for Recall but we have to sacrifice Precision.\n",
    "* Training set **Recall** is 0.92 and test set recall is around 0.875 - a little bit of improvement.\n",
    "* Precision and F1 scores have gone down with this new threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9929f1e",
   "metadata": {},
   "source": [
    "### Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_train_perf.T,\n",
    "        log_reg_model_train_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_train_perf_threshold_curve.T\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_train_comp_df.columns = [\n",
    "    \"Logistic Regression sklearn\",\n",
    "    \"Logistic Regression-0.51 Threshold\",\n",
    "    \"Logistic Regression-0.40 Threshold\"\n",
    "]\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df.T.sort_values(by='Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f78d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_test_perf.T,\n",
    "        log_reg_model_test_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_test_perf_threshold_curve.T\n",
    "        ],\n",
    "    axis=1,\n",
    ")\n",
    "models_test_comp_df.columns = [\n",
    "    \"Logistic Regression sklearn\",\n",
    "    \"Logistic Regression-0.51 Threshold\",\n",
    "    \"Logistic Regression-0.40 Threshold\"\n",
    "]\n",
    "print(\"Test set performance comparison:\")\n",
    "models_test_comp_df.T.sort_values(by='Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e967bc9",
   "metadata": {},
   "source": [
    "### Logistic Regression with Sequential Feature Selector\n",
    "\n",
    "- Reduces dimensionality.\n",
    "- Discards deceptive features (Deceptive features appear to aid learning on the training set, but impair generalization).\n",
    "- Speeds training/testing.\n",
    "\n",
    "Let's see with SFS what are the most important features and if we can get a better recall score using SFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlxtend\n",
    "# Sequential feature selector is present in mlxtend library\n",
    "# !pip install mlxtend to install mlxtent library\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# to plot the performance with addition of each feature\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit the model on train\n",
    "model = LogisticRegression(solver=\"newton-cg\", n_jobs=-1, random_state=1, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will first build model with all varaible\n",
    "sfs = SFS(\n",
    "    model,\n",
    "    k_features=15, # it is same as X_train.shape\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"recall\",\n",
    "    verbose=2,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_sfs(sfs.get_metric_dict(), kind=\"std_dev\", figsize=(12, 5))\n",
    "#plt.ylim([0.8, 1])\n",
    "plt.title(\"Sequential Forward Selection (w. StdDev)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11371d",
   "metadata": {},
   "source": [
    "* We can see that performance increases till the 10th feature and then became constant.\n",
    "* So we'll use 10 features only to build our mode but the choice of k_features it depends on the business context and use case of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1 = SFS(\n",
    "    model,\n",
    "    k_features=10,\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"recall\",\n",
    "    verbose=2,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "sfs1 = sfs1.fit(X_train, y_train)\n",
    "\n",
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind=\"std_dev\", figsize=(10, 5))\n",
    "\n",
    "#plt.ylim([0.8, 1])\n",
    "plt.title(\"Sequential Forward Selection (w. StdDev)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560798e",
   "metadata": {},
   "source": [
    "**Finding which features are important?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cac45",
   "metadata": {},
   "source": [
    "**Let's look at best 10 variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8821ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2999f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train[X_train.columns[feat_cols]]\n",
    "\n",
    "# Creating new x_test with the same variables that we selected for x_train\n",
    "X_test_final = X_test[X_train_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7412412",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_sfs = LogisticRegression(solver=\"newton-cg\", random_state=1, class_weight={0: 0.1, 1: 0.9})\n",
    "lg_sfs.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00c4dc",
   "metadata": {},
   "source": [
    "### Let's Look at model performance with SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec1df3",
   "metadata": {},
   "source": [
    "#### Training set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix_with_threshold(lg_sfs, X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb030d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf_SFS = show_model_perf_with_threshold(lg_sfs, X_train_final, y_train)\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcbc6c",
   "metadata": {},
   "source": [
    "#### Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46eda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix_with_threshold(lg_sfs, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf_SFS = show_model_perf_with_threshold(lg_sfs, X_test_final, y_test)\n",
    "print(\"Test set performance:\")\n",
    "log_reg_model_test_perf_SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82293e7f",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    "* Model is giving a generalized performance on training and test set.\n",
    "* With a fewer number of features, the model performance is comparable to the initial logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade3c57",
   "metadata": {},
   "source": [
    "### Model Performance Summary with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_train_perf.T,\n",
    "        log_reg_model_train_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_train_perf_threshold_curve.T,\n",
    "        log_reg_model_train_perf_SFS.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_train_comp_df.columns = [\n",
    "    \"Logistic Regression sklearn\",\n",
    "    \"Logistic Regression-0.51 Threshold\",\n",
    "    \"Logistic Regression-0.40 Threshold\",\n",
    "    \"Logistic Regression - SFS\",\n",
    "]\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df.T.sort_values(by='Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_test_perf.T,\n",
    "        log_reg_model_test_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_test_perf_threshold_curve.T,\n",
    "        log_reg_model_test_perf_SFS.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_test_comp_df.columns = [\n",
    "    \"Logistic Regression sklearn\",\n",
    "    \"Logistic Regression-0.51 Threshold\",\n",
    "    \"Logistic Regression-0.40 Threshold\",\n",
    "    \"Logistic Regression - SFS\",\n",
    "]\n",
    "print(\"Test set performance comparison:\")\n",
    "models_test_comp_df.T.sort_values(by='Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353e5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84055e0c",
   "metadata": {},
   "source": [
    "## Build Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585f84f",
   "metadata": {},
   "source": [
    "* We will build a model using the DecisionTreeClassifier function. Using default 'gini' criteria to split. \n",
    "* If the frequency of class A (Personal Loan:1) is 9% and the frequency of class B (Personal Loan:0) is 91%, then class B will become the dominant class and the decision tree will become biased toward the dominant classes.\n",
    "\n",
    "* In this case, we can pass a dictionary {0:0.1,1:0.9} to the model to specify the weight of each class and the decision tree will give more weightage to class 1.\n",
    "\n",
    "* class_weight is a hyperparameter for the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bc76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to build decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# To get diferent metric scores\n",
    "from sklearn.metrics import (\n",
    "    make_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", class_weight={0: 0.10, 1: 0.90}, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfe9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b063a",
   "metadata": {},
   "source": [
    "#### Create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
    "* The get_recall_score function will be used to check the model performance of decision tree models. \n",
    "* The make_confusion_matrix function will be used to plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97549254",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate recall score\n",
    "def get_recall_score(model, predictors, target):\n",
    "    \"\"\"\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \n",
    "    \"\"\"\n",
    "    prediction = model.predict(predictors)\n",
    "    return recall_score(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bed137",
   "metadata": {},
   "source": [
    "#### Checking model performance on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_perf_train = get_recall_score(model, X_train, y_train)\n",
    "print(\"Recall Score:\", decision_tree_perf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef511c08",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* As predicted with default paramters, Model is able to perfectly classify all the data points on the training set.\n",
    "* 0 errors on the training set, each sample has been classified correctly.\n",
    "* As we know a decision tree will continue to grow and classify each data point correctly if no restrictions are applied as the trees will learn all the patterns in the training set.\n",
    "* This generally leads to overfitting of the model as Decision Tree will perform well on the training set but will fail to replicate the performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fe61e",
   "metadata": {},
   "source": [
    "#### Checking model performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c022a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd17371",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_perf_test = get_recall_score(model, X_test, y_test)\n",
    "print(\"Recall Score:\", decision_tree_perf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0039b",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* Recall score for the test set is not bad but there may be improvement oppertunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86187e49",
   "metadata": {},
   "source": [
    "## Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a list of column names\n",
    "feature_names = X_train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec06d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to plot tree with some custom arguments so that this code can be reused later\n",
    "def plot_tree(model, feature_names, figsize=(15,10), fontsize=12):\n",
    "    \"\"\"\n",
    "    Plots a tree from a Decision Tree model and features. \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    out = tree.plot_tree(\n",
    "        model,\n",
    "        feature_names=feature_names,\n",
    "        filled=True,\n",
    "        fontsize=fontsize,\n",
    "        node_ids=False,\n",
    "        rounded=True,\n",
    "        class_names=None,\n",
    "    )\n",
    "    # below code will add arrows to the decision tree split if they are missing\n",
    "    for o in out:\n",
    "        arrow = o.arrow_patch\n",
    "        if arrow is not None:\n",
    "            arrow.set_edgecolor(\"black\")\n",
    "            arrow.set_linewidth(1)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ed69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model, feature_names, figsize=(20,30), fontsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b15c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "print(tree.export_text(model, feature_names=feature_names, show_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "imp_df1 = pd.DataFrame(\n",
    "        model.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n",
    "    ).sort_values(by=\"Imp\", ascending=False)\n",
    "imp_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e47e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a plot with relative importance where importance is > 0\n",
    "plt.figure(figsize=(12, 10))\n",
    "data = imp_df1[imp_df1.Imp > 0]\n",
    "sns.barplot(x=data.Imp, y=data.index, label=\"Relative Importance\", palette=\"magma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5faca1e",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* Decision Tree model has given very high importance to the Income feature\n",
    "* Family size, Education and CCAvg also carry high importance.\n",
    "* Age, Mortgage, CD_Account and Couties with median Income between 60K to 70K has some importance.\n",
    "* Other features have very low importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6747d",
   "metadata": {},
   "source": [
    "### Finding the best possible model with Pre Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeae791",
   "metadata": {},
   "source": [
    "Althogh we got better recall score on test set than Logistic Regression with default Decision Tree parameters, but we would like to see if the model can be generalized more to get a better recall score with test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603c981",
   "metadata": {},
   "source": [
    "#### Using GridSearch for Hyperparameter tuning of our tree model\n",
    "\n",
    "* Hyperparameter tuning is also tricky in the sense that there is no direct way to calculate how a change in the\n",
    "  hyperparameter value will reduce the loss of your model, so we usually resort to experimentation. i.e we'll use Grid search\n",
    "* Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \n",
    "* It is an exhaustive search that is performed on a the specific parameter values of a model.\n",
    "* The parameters of the estimator/model used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "estimator = DecisionTreeClassifier(random_state=1, class_weight={0: 0.10, 1: 0.90})\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"min_impurity_decrease\": [0.00001, 0.0001, 0.01],\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = make_scorer(recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(estimator, parameters, scoring=scorer, cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "estimator = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dc81e",
   "metadata": {},
   "source": [
    "#### Checking performance on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4143bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(estimator, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tune_perf_train = get_recall_score(estimator, X_train, y_train)\n",
    "print(\"Recall Score:\", decision_tree_tune_perf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c1cda",
   "metadata": {},
   "source": [
    "#### Checking performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(estimator, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d99c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tune_perf_test = get_recall_score(estimator, X_test, y_test)\n",
    "print(\"Recall Score:\", decision_tree_tune_perf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93b1fc",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* The model is giving a generalized result now.\n",
    "* Test recall score is around 0.986"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c8551",
   "metadata": {},
   "source": [
    "## Visualizing the GV Optimized Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb413e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(estimator, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "# (normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "imp_df2 =  pd.DataFrame(\n",
    "        estimator.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n",
    "    ).sort_values(by=\"Imp\", ascending=False)\n",
    "\n",
    "imp_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a plot with relative importance where importance is > 0\n",
    "plt.figure(figsize=(12, 4))\n",
    "data = imp_df2[imp_df2.Imp > 0]\n",
    "sns.barplot(x=data.Imp, y=data.index, label=\"Relative Importance\", palette=\"magma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a51c04",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    "\n",
    "* The pre-pruned Decision Tree has only five level including the root\n",
    "* It only considers four features Income, Family, Education, CCAge in order of importance.\n",
    "* Other features do not impact the decision making of the pruned tree.\n",
    "\n",
    "`So it seems customers having higher income, family size, higher education and higher monthly credit card usage will likely to buy a Personal Loan product.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f633d4",
   "metadata": {},
   "source": [
    "## Cost Complexity Pruning - Post Pruning\n",
    "\n",
    "Although we have found a very good recall score with earlier Pre Pruned Tree but we need to see how the recall score changes with Post Pruning techniques.\n",
    "\n",
    "\n",
    "The `DecisionTreeClassifier` provides parameters such as\n",
    "``min_samples_leaf`` and ``max_depth`` to prevent a tree from overfiting. Cost\n",
    "complexity pruning provides another option to control the size of a tree. In\n",
    "`DecisionTreeClassifier`, this pruning technique is parameterized by the\n",
    "cost complexity parameter, ``ccp_alpha``. Greater values of ``ccp_alpha``\n",
    "increase the number of nodes pruned. Here we only show the effect of\n",
    "``ccp_alpha`` on regularizing the trees and how to choose a ``ccp_alpha``\n",
    "based on validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1b8a2",
   "metadata": {},
   "source": [
    "Total impurity of leaves vs effective alphas of pruned tree\n",
    "---------------------------------------------------------------\n",
    "Minimal cost complexity pruning recursively finds the node with the \"weakest\n",
    "link\". The weakest link is characterized by an effective alpha, where the\n",
    "nodes with the smallest effective alpha are pruned first. To get an idea of\n",
    "what values of ``ccp_alpha`` could be appropriate, scikit-learn provides\n",
    "`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the\n",
    "effective alphas and the corresponding total leaf impurities at each step of\n",
    "the pruning process. As alpha increases, more of the tree is pruned, which\n",
    "increases the total impurity of its leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1, class_weight={0: 0.10, 1: 0.90})\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23052ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41abde06",
   "metadata": {},
   "source": [
    "Next, we train a decision tree using the effective alphas. The last value\n",
    "in ``ccp_alphas`` is the alpha value that prunes the whole tree,\n",
    "leaving the tree, ``clfs[-1]``, with one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87186b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(\n",
    "        random_state=1, ccp_alpha=ccp_alpha, class_weight={0: 0.10, 1: 0.90}\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de38733",
   "metadata": {},
   "source": [
    "\n",
    "For the remainder, we remove the last element in\n",
    "``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one\n",
    "node. Here we show that the number of nodes and tree depth decreases as alpha\n",
    "increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 7))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_train = []\n",
    "for clf in clfs:\n",
    "    pred_train = clf.predict(X_train)\n",
    "    values_train = recall_score(y_train, pred_train)\n",
    "    recall_train.append(values_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_test = []\n",
    "for clf in clfs:\n",
    "    pred_test = clf.predict(X_test)\n",
    "    values_test = recall_score(y_test, pred_test)\n",
    "    recall_test.append(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1da57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.set_title(\"Recall vs alpha for training and testing sets\")\n",
    "ax.plot(\n",
    "    ccp_alphas, recall_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\",\n",
    ")\n",
    "ax.plot(ccp_alphas, recall_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model where we get highest train and test recall\n",
    "index_best_model = np.argmax(recall_test)\n",
    "best_model = clfs[index_best_model]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa17037",
   "metadata": {},
   "source": [
    "#### Check performance on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcd77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e45ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall Score:\", get_recall_score(best_model, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ae2ed",
   "metadata": {},
   "source": [
    "#### Check performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3745df",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall Score:\", get_recall_score(best_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05531ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(best_model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c07fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n",
    "\n",
    "imp_df3 = pd.DataFrame(\n",
    "        best_model.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n",
    "    ).sort_values(by=\"Imp\", ascending=False)\n",
    "imp_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "data = imp_df3[imp_df3.Imp > 0]\n",
    "sns.barplot(x=data.Imp, y=data.index, label=\"Relative Importance\", palette=\"magma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb587bf",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "\n",
    "* Post pruned Tree with optimized CCP alpha is giving a recall score of 0.986\n",
    "* It also give very high importance to Income followed by Family size, higher Education and higher CCAvg.\n",
    "* Also the Recall score is similar to Pre Pruned tree with 5 levels.\n",
    "\n",
    "Although we have achieved a very good recall score and it matches with pre-pruned tree but the training recall score is 1.0 which is a sign of little overfitting. For completeness we woulod check with a higher alpha to generalize the tree a bit more but making sure Recall score does not degrade.\n",
    "\n",
    "From the **Recall vs alpha for training and testing sets** chart we see that recall score does not drop till about alpha = 0.025, hence we can go for a more generous alpha value and this will generalize the tree even more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece218e5",
   "metadata": {},
   "source": [
    "**Creating model with 0.025 ccp_alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aabd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model2 = DecisionTreeClassifier(\n",
    "    ccp_alpha=0.025, class_weight={0: 0.10, 1: 0.90}, random_state=1\n",
    ")\n",
    "best_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e828a",
   "metadata": {},
   "source": [
    "#### Check performance on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aeed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_postpruned_perf_train = get_recall_score(best_model2, X_train, y_train)\n",
    "print(\"Recall Score:\", decision_tree_postpruned_perf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f7b33",
   "metadata": {},
   "source": [
    "#### Check performance on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_postpruned_perf_test = get_recall_score(best_model2, X_test, y_test)\n",
    "print(\"Recall Score:\", decision_tree_postpruned_perf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(best_model2, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "print(tree.export_text(best_model2, feature_names=feature_names, show_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b100d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building ( The importance of a feature is computed as the\n",
    "# (normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )\n",
    "imp_df4 = pd.DataFrame(\n",
    "        best_model2.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n",
    "    ).sort_values(by=\"Imp\", ascending=False)\n",
    "\n",
    "imp_df4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "data = imp_df4[imp_df4.Imp > 0]\n",
    "sns.barplot(x=data.Imp, y=data.index, label=\"Relative Importance\", palette=\"magma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a9058",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "\n",
    "* With ccp alpha 0.025 we are getting the same recall score with ccp alpha 0.004. But the tree is depth is less with alpha 0.025.\n",
    "* Other observations related to feature importance remain same.\n",
    "* With ccp alpha 0.025, the Decision Tree is same as the earlier Pre Pruned tree with max depth = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccabc87",
   "metadata": {},
   "source": [
    "### Comparing all the Decision Tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.DataFrame(\n",
    "    [\n",
    "        decision_tree_perf_train,\n",
    "        decision_tree_tune_perf_train,\n",
    "        decision_tree_postpruned_perf_train,\n",
    "    ],\n",
    "    index=['decision_tree_perf_train', 'decision_tree_prepruned_perf_train', 'decision_tree_postpruned_perf_train'],\n",
    "    columns=[\"Recall on training set\"]\n",
    ")\n",
    "\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_test_comp_df = pd.DataFrame(\n",
    "    [\n",
    "        decision_tree_perf_test,\n",
    "        decision_tree_tune_perf_test,\n",
    "        decision_tree_postpruned_perf_test,\n",
    "    ],\n",
    "    index=['decision_tree_perf_test', 'decision_tree_prepruned_perf_test', 'decision_tree_postpruned_perf_test'],\n",
    "    columns=[\"Recall on test set\"]\n",
    ")\n",
    "\n",
    "print(\"Test performance comparison:\")\n",
    "models_test_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106dcee8",
   "metadata": {},
   "source": [
    "> **Observations**:\n",
    "* Pre Pruned Decision Tree has comparable Train and Test performance.\n",
    "* Post Pruned Decision Tree also has comparable Train and Test performance.\n",
    "* Pre Pruned Decision Tree and Post Pruned Decision Tree have same Recall score and tree depth and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d2b67",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df8875",
   "metadata": {},
   "source": [
    "- We analyzed the data of AllLife's Bank Personal Loan campaign using both Logistic Regression models and Decision Tree Classifiers to build various predictive models to better understand the features that may have influenced the campaign result. \n",
    "\n",
    "- Based on the business case we also decided that the Recall score should the performance criteria of our analysis, since we want to reduce the False Nagatives as oppertunity cost would be higher. In this case we are not very worried about the precision score since the cost of a campaign such as direct mail marketting or emails are not significant. Based on these analysis below conclusions can be drawn :-\n",
    "\n",
    "> **Logisctic Regression**\n",
    "\n",
    "- Logisctic Regression does not completely ignore features. Almost all features influnce the outcome but the degree of influence varies greatly.\n",
    "- Out of all the featues if a customer is having CD_Account, higher Education, bigger Family, higher monthly CCAvg, higher income Income, Age, Mortgage then he/she is more likely to opt for a Personal Loan. \n",
    "- On the other hand if a customer is having Credit cards with other banks, Securities accounts or uses Internet banking facilities then the probability of taking a Personal Loan decreses.\n",
    "- Based on the customer location we can draw a conclusion that customers from counties having median income between 60K to 80K have a higher probablity of buying personal loan than other counties. \n",
    "\n",
    "> **Decision Tree**\n",
    "\n",
    "- Although there is some overlap in terms of conclusions but the Decision Tree gives a different perspective and ignores many features while predicting a Personal Loan customer. \n",
    "- It gives the highest importance to Income. It clearly concludes that if a a cusomer has a higher income, he/she is more likely to go for a Personal Loan.\n",
    "- It also gives weightage to bigger Family size, higher Education and higher monthly average Credit crad usage. Customers matching these criterions also have higher probability of buying personal loan products.\n",
    "- Interestingly, Decision Tree does not give any importance to features other then Income, Family, Education and CCAvg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159666c",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b55f22",
   "metadata": {},
   "source": [
    "Based on the combined prediction results of both Logistic Regression and Decision Tree classification models and conclusions we offer below recommendations to AllLife Bank marketing department Personal Loan campaign.\n",
    "\n",
    "- Internet research shows people take personal loans for covering cost of emergency expense, wedding, vacation, appliance purchase, home remodelling etc. Considering the repayment capability of a customer we should target high income range costomers first as this feature has come up in both types of models and from a bank perspective these will be safer investments.\n",
    "\n",
    "- Cusotmers having CD accounts with the bank could also be likly canditates for Personal loans. As they would have some surity as well for paying back.\n",
    "\n",
    "- Customers with bigger family size such 3 or 4 members, higher Education such as graduate or advanced degrees, higher monthly Credit card usage can be targetted next. \n",
    "\n",
    "- Also customers having higher morgages, age and professional experience can be targetted for personal loans but the conversion rate may be low here.\n",
    "\n",
    "- In order to increase the conversion rate further a personal loan social media campaing can be targetted to ZIP codes in counties where median income is between 60K to 80K per year. As analysis has shown people from these counties have higher probability of buying personal loans.\n",
    "\n",
    "- If customers have credit cards from other banks and lower monthly average credit card usage then they have less probability of buying personal loans as they enjoy high borrowing power already. This type of customers should be targetted less unless other features recommend otherwise.\n",
    "\n",
    "With above recommendations, derived from both Logistic Regression and Decision Tree models, we believe conversion rate of Personal Loan campaign will increase from the current rate of 9% to higher and AllLife Banks will be better equipped in converting its liability customers to personal loan customers (while retaining them as depositors).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
